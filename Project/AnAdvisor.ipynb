{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "iZys7o-BTpK3",
      "metadata": {
        "id": "iZys7o-BTpK3"
      },
      "source": [
        "# AnAdvisor\n",
        "AnAdvisor is a service for anime recommendation based on user's preferences\n",
        "\n",
        "Once the user submit their own list, the system will automatically profile them considering what they already watched, and provide recommendation relying on that\n",
        "\n",
        "The approach i will follow is to cluster similiar users togheter based on the mean score they given to each genre, and then provide the best anime seen by people in the same cluster of the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22af7373",
      "metadata": {
        "id": "22af7373"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import random as rm\n",
        "import math\n",
        "import gc\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "root = './Datasets'\n",
        "animeFile = root + '/anime.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75bbbdf3",
      "metadata": {
        "id": "75bbbdf3"
      },
      "outputs": [],
      "source": [
        "anime = pd.read_csv(animeFile, na_values=['Unknown'])\n",
        "anime.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FeOUT7Vis1i5",
      "metadata": {
        "id": "FeOUT7Vis1i5"
      },
      "outputs": [],
      "source": [
        "#----------MEMORY CLEANING------------\n",
        "del animeFile\n",
        "#-------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KYGTyQWCsNWd",
      "metadata": {
        "id": "KYGTyQWCsNWd"
      },
      "source": [
        "### Datasets details\n",
        "The following datasets have been provided by MAL.net\n",
        "\n",
        "  - Anime\n",
        "    - `MAL_ID`: ***Changed in `anime_id`***\n",
        "    - `Name`\n",
        "    - `Score`: ***Mean score***\n",
        "    - `Genres`: ***List of generes***\n",
        "    - `Type`: ***TV, Film, ...***\n",
        "    - `Episodes`: ***Number of episodes***\n",
        "    - `Members`: ***Number of members of the anime 'group'***\n",
        "    - `Favorites`: ***Number of users who have the anime as 'Favorite'***\n",
        "    - `Other minor attributes`\n",
        "\n",
        "  - Animelist\n",
        "    - `user_id`\n",
        "    - `anime_id`\n",
        "    - `score`: ***Vote given by `user_id` to `anime_id`***\n",
        "    - `watching_status`: ***Display if `anime_id` for `user_id` is in a Watching status, Planning, Dropped and so on***\n",
        "    - `watched_episodes`: ***Number of episodes watched***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n87MqLQ0tZ5j",
      "metadata": {
        "id": "n87MqLQ0tZ5j"
      },
      "source": [
        "# Anime\n",
        "This dataset contains various informations about all available anime (until early 2020)\n",
        "\n",
        "I will make some exploration, and pre processing, to make the dataset usable later, if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pvLaEEecTYnj",
      "metadata": {
        "id": "pvLaEEecTYnj"
      },
      "source": [
        "I decided to remove all unnecessary attributes for the analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33589548",
      "metadata": {
        "id": "33589548"
      },
      "outputs": [],
      "source": [
        "dropCList = ['English name', 'Japanese name', 'Aired', 'Premiered',\n",
        "             'Producers', 'Licensors', 'Studios', 'Duration', 'Rating',\n",
        "             'Source', 'Watching', 'Completed', 'Ranked',\n",
        "             'On-Hold', 'Dropped', 'Plan to Watch', 'Score-10',\n",
        "             'Score-9', 'Score-8', 'Score-7', 'Score-6', 'Score-5',\n",
        "             'Score-4', 'Score-3', 'Score-2', 'Score-1']\n",
        "\n",
        "anime.drop(columns = dropCList, inplace = True)\n",
        "anime.rename(columns = {'MAL_ID': 'anime_id', 'Name': 'name', 'Score': 'score', \n",
        "                     'Genres': 'genres', 'Episodes': 'episodes', 'Type': 'type',\n",
        "                     'Members': 'members', 'Favorites': 'favorites', 'Popularity': 'popularity'},\n",
        "                     inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p27ZIoXmgN3H",
      "metadata": {
        "id": "p27ZIoXmgN3H"
      },
      "source": [
        "I decided to handle all the nan values in different ways"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KwmAcCiykEm8",
      "metadata": {
        "id": "KwmAcCiykEm8"
      },
      "outputs": [],
      "source": [
        "anime.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe01dea",
      "metadata": {
        "id": "8fe01dea"
      },
      "outputs": [],
      "source": [
        "anime.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-BjJxrHKNPbG",
      "metadata": {
        "id": "-BjJxrHKNPbG"
      },
      "source": [
        "Since genres is the most important feature for similarity calculation, i drop all animes without any. I will also drop anime without a type, since it's impossible to manually check and add to all of them\n",
        "  - There are some major shows recently added which have no information, but i'm not interested on keeping them, since they have not been released yet, i can rely on them \n",
        "\n",
        "I also drop 'Music' type becouse it's out of my interest\n",
        "  - This genres contains only best soundtracks of different anime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kACDNBJd4s8f",
      "metadata": {
        "id": "kACDNBJd4s8f"
      },
      "outputs": [],
      "source": [
        "# ------------REMOVE NO GENRE FROM ANIMES---------------\n",
        "mask = anime['genres'].isna()\n",
        "anime.drop(anime.index[mask], inplace = True)\n",
        "# ------------REMOVE NO TYPE FROM ANIMES----------------\n",
        "mask = anime['type'].isna()\n",
        "anime.drop(anime.index[mask], inplace = True)\n",
        "    \n",
        "# ----------REMOVE MUSIC TYPE FROM ANIMES---------------\n",
        "mask = anime['type'] == 'Music'\n",
        "anime.drop(anime.index[mask], inplace = True)\n",
        "# ----------REMOVE THE ONE WITH POPULARITY == 0--------- \n",
        "# It's a dataset error\n",
        "mask = anime['popularity'] == 0\n",
        "anime.drop(anime.index[mask], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A41ui3E9x6vM",
      "metadata": {
        "id": "A41ui3E9x6vM"
      },
      "outputs": [],
      "source": [
        "anime.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EY_y7f_fheNf",
      "metadata": {
        "id": "EY_y7f_fheNf"
      },
      "source": [
        "Let's see with a boxplot the the distribution of missing mean score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6WeWvOPpfMe1",
      "metadata": {
        "id": "6WeWvOPpfMe1"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1, figsize=(2,5), sharey = True)\n",
        "sb.boxplot(y = 'score', data = anime, ax=plot)\n",
        "plot.set_title('Score distribution')\n",
        "plot.xaxis.set_visible(False)\n",
        "plot.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCJgA31Ai1ZH",
      "metadata": {
        "id": "jCJgA31Ai1ZH"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,4, figsize=(15,10), sharey = True)\n",
        "\n",
        "sb.boxplot(y = 'score', data = anime, ax=plot[0])\n",
        "sb.boxplot(data = anime.score.fillna(5), ax=plot[1])\n",
        "sb.boxplot(data = anime.score.fillna(anime.score.mean()), ax=plot[2])\n",
        "sb.boxplot(data = anime.score.fillna(anime.score.median()), ax=plot[3])\n",
        "plot[0].set_title('Original')\n",
        "plot[1].set_title('Replace with 5')\n",
        "plot[2].set_title('Replace with mean')\n",
        "plot[3].set_title('Replace with median')\n",
        "plot[0].xaxis.set_visible(False)\n",
        "plot[1].xaxis.set_visible(False)\n",
        "plot[2].xaxis.set_visible(False)\n",
        "plot[3].xaxis.set_visible(False)\n",
        "plot[0].grid()\n",
        "plot[1].grid()\n",
        "plot[2].grid()\n",
        "plot[3].grid()\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nScore mean: {anime.score.mean()}\\nScore median: {anime.score.median()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R2YyIo9XPTA-",
      "metadata": {
        "id": "R2YyIo9XPTA-"
      },
      "source": [
        "Most of those still have to be aired (i removed 'Aired', but i can still tell becouse most of them don't have episodes either)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ibVBHFB3WJPr",
      "metadata": {
        "id": "ibVBHFB3WJPr"
      },
      "outputs": [],
      "source": [
        "anime.loc[anime.score.isna()].sort_values(by = 'popularity', ascending = True).head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7AD6vhsBW29y",
      "metadata": {
        "id": "7AD6vhsBW29y"
      },
      "source": [
        "I decided to fill all unrated anime with the median"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8wEv_xI5o5FT",
      "metadata": {
        "id": "8wEv_xI5o5FT"
      },
      "outputs": [],
      "source": [
        "anime['score'].fillna(anime.score.median(), inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gN2WFkji2n3o",
      "metadata": {
        "id": "gN2WFkji2n3o"
      },
      "outputs": [],
      "source": [
        "anime.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O2lNBsUQrWfE",
      "metadata": {
        "id": "O2lNBsUQrWfE"
      },
      "source": [
        "I'm considering also to remove any anime with a score lower than 5, i want the system to recommend only decently scored anime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-lIiRn6CP1G9",
      "metadata": {
        "id": "-lIiRn6CP1G9"
      },
      "outputs": [],
      "source": [
        "print(f\"Total anime count: ({anime.shape[0]})\")\n",
        "print(f\"With score greater than 5: ({anime.loc[anime.score >= 5].shape[0]})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1YmY9DxijOer",
      "metadata": {
        "id": "1YmY9DxijOer"
      },
      "source": [
        "Since there are not many anime with a score lower than 5, i will remove them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vgV6hgTFr96X",
      "metadata": {
        "id": "vgV6hgTFr96X"
      },
      "outputs": [],
      "source": [
        "anime = anime[anime['score'] >= 5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y4VoC3JyjcLB",
      "metadata": {
        "id": "Y4VoC3JyjcLB"
      },
      "source": [
        "Now i will create a new attribute for each genre: for each anime, if a given genre has value 1, then this one belongs to that genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SifnQVwwYFt8",
      "metadata": {
        "id": "SifnQVwwYFt8"
      },
      "outputs": [],
      "source": [
        "def unpack(x):\n",
        "  glist = x['genres'].split(',')\n",
        "  for g in glist:\n",
        "    x[g.strip()] = 1\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Nl2853X2P3X",
      "metadata": {
        "id": "4Nl2853X2P3X"
      },
      "outputs": [],
      "source": [
        "genres = anime[['anime_id', 'genres']]\n",
        "genresUnpacked = genres.apply(unpack, axis=1).drop(columns = ['genres'])\n",
        "anime.drop(columns = ['genres'], inplace = True)\n",
        "genresUnpacked.fillna(0, inplace = True)\n",
        "genresUnpacked = genresUnpacked.astype(int)\n",
        "genresUnpacked"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lIxqFwSzkIrf",
      "metadata": {
        "id": "lIxqFwSzkIrf"
      },
      "source": [
        "I want to visualize if there is some correlation between genres in order to perform some dimentionality reduction and get rid of some genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O4f_H6UTkW_D",
      "metadata": {
        "id": "O4f_H6UTkW_D"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(figsize = (20,20))\n",
        "\n",
        "sb.heatmap(genresUnpacked.drop(columns = 'anime_id').corr(), linewidth=3, square = True, ax = plot, annot = True)\n",
        "plt.title('Correlation Matrix', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "hZ4JTe70RPTh",
      "metadata": {
        "id": "hZ4JTe70RPTh"
      },
      "source": [
        "Unfortunately seems that there is no pair of genres with a strong correlation, so i can't remove independent genres\n",
        "\n",
        "I want anyway to remove some of minor genres, those which don't appear many time in the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7-pGqjwyqyvb",
      "metadata": {
        "id": "7-pGqjwyqyvb"
      },
      "outputs": [],
      "source": [
        "genresUnpacked.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h_2Bq5WyrHes",
      "metadata": {
        "id": "h_2Bq5WyrHes"
      },
      "source": [
        "Looking at how many time each genre occurs, i decided to remove each genre that appears less than 400 times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pTJ6UNWKQ7xY",
      "metadata": {
        "id": "pTJ6UNWKQ7xY"
      },
      "outputs": [],
      "source": [
        "print(f\"Genres before removal:\\t{genresUnpacked.shape[1]-1}\")\n",
        "minorGenres = genresUnpacked.columns[~(genresUnpacked.sum() >= 400)]\n",
        "genresUnpacked.drop(columns = minorGenres, inplace = True)\n",
        "print(f\"Genres after removal:\\t{genresUnpacked.shape[1]-1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R0hLpNIcfbKr",
      "metadata": {
        "id": "R0hLpNIcfbKr"
      },
      "outputs": [],
      "source": [
        "# Finally join each anime with genres\n",
        "anime.set_index('anime_id', inplace = True)\n",
        "anime = anime.join(genresUnpacked.set_index('anime_id'))\n",
        "anime.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_uCwg--NIXyP",
      "metadata": {
        "id": "_uCwg--NIXyP"
      },
      "outputs": [],
      "source": [
        "genres = genresUnpacked.drop(columns = ['anime_id']).columns.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s0m3tMpRkm_9",
      "metadata": {
        "id": "s0m3tMpRkm_9"
      },
      "outputs": [],
      "source": [
        "# Check how many anime are now without a genre\n",
        "(genresUnpacked.set_index('anime_id').T.sum() < 1).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_ZeiAPR3lDrm",
      "metadata": {
        "id": "_ZeiAPR3lDrm"
      },
      "source": [
        "As i removed some genres, i also have to remove those anime which are now without any genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YPy1YnH6jVYQ",
      "metadata": {
        "id": "YPy1YnH6jVYQ"
      },
      "outputs": [],
      "source": [
        "anime = anime.loc[anime.index[genresUnpacked.set_index('anime_id').T.sum() > 0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XZPtPZbL5qOn",
      "metadata": {
        "id": "XZPtPZbL5qOn"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del dropCList\n",
        "del genresUnpacked\n",
        "del plot\n",
        "del fig\n",
        "del mask\n",
        "del minorGenres\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WFTQPR_zqJ1g",
      "metadata": {
        "id": "WFTQPR_zqJ1g"
      },
      "source": [
        "Animes without episodes consist in two categories:\n",
        "  - Those still airing but is not known when they will have an end\n",
        "  - Not yet aired or it is unknown how many episodes they will have\n",
        "\n",
        "Those out of ordinary schemes (ie. unusual number of total episodes) must be manually updated to last known episode, while to the others will be assigned the mean based on anime Type (Movies have 1 episode while non-Movies have usually 12 or 24 episodes, i will assign the closest one to the mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zaHrgYRlCBxQ",
      "metadata": {
        "id": "zaHrgYRlCBxQ"
      },
      "outputs": [],
      "source": [
        "# Anime with NaN episode\n",
        "anime[anime.episodes.isna()].sort_values(by = 'members', ascending = False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2JR3q1u9g6U",
      "metadata": {
        "id": "d2JR3q1u9g6U"
      },
      "outputs": [],
      "source": [
        "most_famous = {21:1044, 34566:279, 235:1067, 42205:64}\n",
        "for k,v in most_famous.items():\n",
        "  anime.loc[k, 'episodes'] = v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yLcJjSMN7AQt",
      "metadata": {
        "id": "yLcJjSMN7AQt"
      },
      "outputs": [],
      "source": [
        "print(f\"Movie episodes mean: {anime[anime['type'] == 'Movie'].episodes.mean()}\")\n",
        "print(f\"Special episodes mean: {anime[anime['type'] == 'Special'].episodes.mean()}\")\n",
        "print(f\"TV episodes mean: {anime[anime['type'] == 'TV'].episodes.mean()}\")\n",
        "print(f\"OVA episodes mean: {anime[anime['type'] == 'OVA'].episodes.mean()}\")\n",
        "print(f\"ONA episodes mean: {anime[anime['type'] == 'ONA'].episodes.mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T4oiY6V8nvhq",
      "metadata": {
        "id": "T4oiY6V8nvhq"
      },
      "outputs": [],
      "source": [
        "def fillEpisode(x):\n",
        "  if not(pd.isna(x.episodes)):\n",
        "    return x\n",
        "  if x.type == 'Movie':\n",
        "    x.episodes = 1\n",
        "  else:\n",
        "    # For each non-Movie i will assign 12\n",
        "    x.episodes = 12\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2z9MYUcQmuLK",
      "metadata": {
        "id": "2z9MYUcQmuLK"
      },
      "outputs": [],
      "source": [
        "anime = anime.apply(fillEpisode, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gInG2R_-mB_b",
      "metadata": {
        "id": "gInG2R_-mB_b"
      },
      "outputs": [],
      "source": [
        "anime.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ghUz06Odkt-D",
      "metadata": {
        "id": "ghUz06Odkt-D"
      },
      "outputs": [],
      "source": [
        "anime.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QsWNuvQuiqxr",
      "metadata": {
        "id": "QsWNuvQuiqxr"
      },
      "source": [
        "# Some Analytics on Anime dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ZtO4kJ2KJju",
      "metadata": {
        "id": "9ZtO4kJ2KJju"
      },
      "outputs": [],
      "source": [
        "genresCount = pd.DataFrame(columns = ['Genre', 'Count'])\n",
        "for x in genres:\n",
        "  genresCount = genresCount.append({'Genre':x, 'Count':anime[anime[x] == 1].name.count()}, ignore_index=True)\n",
        "fig, plot = plt.subplots(1,1, figsize = (15,10))\n",
        "sb.barplot(data = genresCount.sort_values(by = 'Count', ascending = False).head(15), x = 'Genre', y = 'Count', ax = plot)\n",
        "sb.set(font_scale=1.8)\n",
        "plt.tick_params(axis='x', rotation=65)\n",
        "plt.title('Number of anime per genre')\n",
        "plot.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hGP133efMw4F",
      "metadata": {
        "id": "hGP133efMw4F"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1, figsize = (15,10))\n",
        "sb.barplot(data = anime.sort_values(by = 'members', ascending = False).head(15), x = 'name', y = 'members', ax = plot)\n",
        "sb.set(font_scale=1.5)\n",
        "plt.tick_params(axis='x', rotation=90)\n",
        "plt.title('Top 15 watched animes')\n",
        "plot.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7m6bCoL8lQuq",
      "metadata": {
        "id": "7m6bCoL8lQuq"
      },
      "outputs": [],
      "source": [
        "scoreXType = anime[['type', 'score']]\n",
        "# I will round scores to nearest natural: round up if decimal is grater than 0.5, round down otherwise\n",
        "scoreXType.score = scoreXType.score.apply(lambda x: np.ceil(x) if(x - np.floor(x) > 0.5) else np.floor(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zuw15e0Fpjky",
      "metadata": {
        "id": "zuw15e0Fpjky"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,figsize=(15,5))\n",
        "sb.countplot(x='score', hue = 'type', data = scoreXType, ax = plot)\n",
        "plot.set_title('Score count per type')\n",
        "plt.legend(loc = 'upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4Vfk3SDunj2",
      "metadata": {
        "id": "b4Vfk3SDunj2"
      },
      "source": [
        "TV has the best trend on 8 and 9 scores, compared to the others. In the opposite way, OVA has a major trand on 5, 6 and 7 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7z0ujYLaj3nz",
      "metadata": {
        "id": "7z0ujYLaj3nz"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del fig\n",
        "del genresCount\n",
        "del k\n",
        "del most_famous\n",
        "del plot\n",
        "del v\n",
        "del x\n",
        "del scoreXType\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lqEMzeDdzvwt",
      "metadata": {
        "id": "lqEMzeDdzvwt"
      },
      "outputs": [],
      "source": [
        "#----------------CHECKPOINT---------------\n",
        "anime.to_csv(root + '/animeCheckpoint.csv')\n",
        "#-----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kNdEHn3lXfhv",
      "metadata": {
        "id": "kNdEHn3lXfhv"
      },
      "source": [
        "# Ratings"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "kJMquGFd1Xvr",
      "metadata": {
        "id": "kJMquGFd1Xvr"
      },
      "source": [
        "In order to keep consistency with the pre-processing i made on Anime dataset, i have to remove all those ratings with an `anime_id` not in the previous dataset\n",
        "\n",
        "I'm also removing those ratings without a vote (rating of 0), or those with a vote but set as Planning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IA2rrfhqq7Jw",
      "metadata": {
        "id": "IA2rrfhqq7Jw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import random as rm\n",
        "import math\n",
        "import gc\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "root = './Datasets'\n",
        "anime = pd.read_csv(root + '/animeCheckpoint.csv', index_col='anime_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jvyb5plEuQ0T",
      "metadata": {
        "id": "jvyb5plEuQ0T"
      },
      "outputs": [],
      "source": [
        "cols = ['user_id','anime_id','rating','watching_status']\n",
        "# Inizialize ratings table for append\n",
        "size = 0\n",
        "ratings = pd.read_csv(root + '/animelist.csv', nrows = 0, usecols = cols)\n",
        "# Since animelist.csv is way too big, i have to perform numerosity reduction in chunks\n",
        "for chunk in pd.read_csv(root + '/animelist.csv', chunksize = 1_000_000, usecols = cols):\n",
        "  # Mask of ratings of anime not in Anime dataset\n",
        "  size += chunk.shape[0]\n",
        "  mask = (~chunk.anime_id.isin(anime.index))\n",
        "  chunk.drop(chunk.index[mask], inplace = True)\n",
        "  # Keep only ratings with score greater than 0\n",
        "  chunk = chunk[chunk['rating'] > 0]\n",
        "  # Keep only ratings with a watching_status < 6 (everything but Planning)\n",
        "  chunk = chunk[chunk['watching_status'] < 6]\n",
        "  # Watching_status encoding {1: 'Watching', 2: 'Completed', 3: 'On-Hold', 4: 'Dropped', 6: 'Planning'}\n",
        "  ratings = ratings.append(chunk, ignore_index = True)\n",
        "ratings.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aUHyTtcd1Za",
      "metadata": {
        "id": "0aUHyTtcd1Za"
      },
      "outputs": [],
      "source": [
        "#----------------CHECKPOINT---------------\n",
        "ratings.to_csv(root + '/ratingsCheckpoint.csv')\n",
        "#-----------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EGzCxQJixerZ",
      "metadata": {
        "id": "EGzCxQJixerZ"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del chunk\n",
        "del cols\n",
        "del mask\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TozOzvakp8tV",
      "metadata": {
        "id": "TozOzvakp8tV"
      },
      "source": [
        "---------------------------------------\n",
        "AT THIS POINT ITS BETTER IF YOU RESTART THE RUNTIME AND START OVER WITH CHEKPOINT DATASETS (MEMORY LEAK PROBLEMS)\n",
        "\n",
        "---------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "naztGiqs7hcH",
      "metadata": {
        "id": "naztGiqs7hcH"
      },
      "outputs": [],
      "source": [
        "%pip install pyclustertend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vS0jsDyCeN_2",
      "metadata": {
        "id": "vS0jsDyCeN_2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 50)\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import random as rm\n",
        "import math\n",
        "import gc\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans, DBSCAN, OPTICS, AgglomerativeClustering\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from pyclustertend import hopkins\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "anime = pd.read_csv(root + '/animeCheckpoint.csv', index_col='anime_id')\n",
        "ratings = pd.read_csv(root + '/ratingsCheckpoint.csv')\n",
        "ratings.drop(columns = ['Unnamed: 0'], inplace = True)\n",
        "# {1: 'Watching', 2: 'Completed', 3: 'On-Hold', 4: 'Dropped', 6: 'Planning'}\n",
        "genres = anime.drop(columns = ['episodes', 'type','name','score','popularity','members','favorites']).columns.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fU5vhASodK3H",
      "metadata": {
        "id": "fU5vhASodK3H"
      },
      "source": [
        "Let's do some Data Cleaning on ratings dataset:\n",
        "  - We are not interested in keeping users who have only 1 anime rated\n",
        "  - We are either not interested in keeping users who have rated tons of anime, this will bias users' cluster too much\n",
        "\n",
        "\n",
        "  After cleaning, i will try to see if they can be splitted into 'groups' of preferred genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xEOQ3-VTZarX",
      "metadata": {
        "id": "xEOQ3-VTZarX"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1, figsize = (15,5))\n",
        "\n",
        "ratingsPerUser = pd.DataFrame(ratings.groupby(['user_id'])['anime_id'].count()).rename(columns = {'anime_id': 'count'})\n",
        "sb.histplot(ratingsPerUser.sort_values(by = 'count', ascending = False), x = 'count', kde = True, ax = plot)\n",
        "plt.xlabel('# Anime rated')\n",
        "plt.tick_params(axis = 'x', rotation = 45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jxYHtiH8y5mj",
      "metadata": {
        "id": "jxYHtiH8y5mj"
      },
      "source": [
        "Seems like someone rated 12k to 14k anime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vXx9vfwGfoav",
      "metadata": {
        "id": "vXx9vfwGfoav"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,2, figsize = (15,5))\n",
        "\n",
        "sb.histplot(ratingsPerUser.loc[(ratingsPerUser['count'] < 50)], x = 'count', kde = True, ax = plot[0])\n",
        "sb.histplot(ratingsPerUser.loc[(ratingsPerUser['count'] > 1500)], x = 'count', kde = True, ax = plot[1])\n",
        "plot[0].set_xlabel('# Anime rated')\n",
        "plot[1].set_xlabel('# Anime rated')\n",
        "\n",
        "fig.suptitle('Head and tail ratings count', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mpAE5nZ-ZViM",
      "metadata": {
        "id": "mpAE5nZ-ZViM"
      },
      "source": [
        "In first place i will remove people with more than 2000 ratings or less than 250:\n",
        "  - People with lot of ratings are considered noise becouse they see a lot of things and don't have a preferred genre; there are also accounts that just random vote every anime\n",
        "  - Also people without not many ratings are not included in the model: their anime list may be a 'subset' of other users. They are also the target that will use the recommendation system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDCEaWMdlAFv",
      "metadata": {
        "id": "hDCEaWMdlAFv"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1, figsize = (15,5))\n",
        "higherOffset = 2000\n",
        "lowerOffset = 250\n",
        "\n",
        "print(f\"Initial users: {ratingsPerUser.shape[0]}\\n\")\n",
        "ratingsPerUser = ratingsPerUser[ratingsPerUser['count'] < higherOffset]\n",
        "ratingsPerUser = ratingsPerUser[ratingsPerUser['count'] > lowerOffset]\n",
        "sb.histplot(ratingsPerUser.sort_values(by = 'count', ascending = False), x = 'count',kde = True, ax = plot)\n",
        "plt.xlabel('# Anime rated')\n",
        "plt.ylabel('Users count')\n",
        "\n",
        "fig.suptitle('How many users per ratings?', fontsize=16)\n",
        "\n",
        "plt.show()\n",
        "print(f\"\\nRemaining users: {ratingsPerUser.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c162HhPCOLIq",
      "metadata": {
        "id": "c162HhPCOLIq"
      },
      "outputs": [],
      "source": [
        "print(f\"Ratings after:\\t{ratings.shape[0]}\")\n",
        "ratings = ratings[ratings.user_id.isin(ratingsPerUser.index)]\n",
        "print(f\"Ratings before:\\t{ratings.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aPTjRfWamC6q",
      "metadata": {
        "id": "aPTjRfWamC6q"
      },
      "source": [
        "We almost halved the Ratings dataset\n",
        "\n",
        "Repeat same process for anime: \n",
        "  - In this step, are removed unpopular anime and those who still have to be released, as they have low number of ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RZf45hTylxF5",
      "metadata": {
        "id": "RZf45hTylxF5"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1, figsize = (15,5))\n",
        "\n",
        "ratingsPerAnime = pd.DataFrame(ratings.groupby(['anime_id'])['user_id'].count()).rename(columns = {'user_id': 'count'})\n",
        "sb.histplot(ratingsPerAnime.sort_values(by = 'count').head(7000), x = 'count', kde = True, ax = plot)\n",
        "plt.xlabel('Number of Ratings')\n",
        "plt.ylabel('Anime count')\n",
        "\n",
        "fig.suptitle('Number of anime per ratings count', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FL1fjEY-oBbJ",
      "metadata": {
        "id": "FL1fjEY-oBbJ"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1, figsize = (15,5))\n",
        "offset = 50\n",
        "\n",
        "print(f\"Initial animes: {ratingsPerAnime.shape[0]}\\n\")\n",
        "ratingsPerAnime = ratingsPerAnime[ratingsPerAnime['count'] >= offset]\n",
        "sb.histplot(ratingsPerAnime.sort_values(by = 'count').head(7000), x = 'count', kde = True, ax = plot)\n",
        "plt.xlabel('Anime rated')\n",
        "plt.tick_params(axis = 'x', rotation = 45)\n",
        "plt.show()\n",
        "print(f\"\\nRemaining animes: {ratingsPerAnime.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O1a0Bwfp226I",
      "metadata": {
        "id": "O1a0Bwfp226I"
      },
      "source": [
        "Definetly remove users and anime from Anime and Ratings datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AI7riBQkr3tq",
      "metadata": {
        "id": "AI7riBQkr3tq"
      },
      "outputs": [],
      "source": [
        "print(f\"Anime before:\\t{anime.shape[0]} | Ratings before:\\t{ratings.shape[0]}\")\n",
        "anime = anime[anime.index.isin(ratingsPerAnime.index)]\n",
        "ratings = ratings[ratings.anime_id.isin(ratingsPerAnime.index)]\n",
        "print(f\"Anime after:\\t{anime.shape[0]} | Ratings after:\\t{ratings.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SwqE9khJsFNe",
      "metadata": {
        "id": "SwqE9khJsFNe"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del fig\n",
        "del higherOffset\n",
        "del lowerOffset\n",
        "del offset\n",
        "del plot\n",
        "del ratingsPerAnime\n",
        "del ratingsPerUser\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fMnV62LS33YI",
      "metadata": {
        "id": "fMnV62LS33YI"
      },
      "source": [
        "Since the ratings dataset is too big even with numerosity reduction, i have to create the table i will use for clustering in chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zCHqqqoxgW-2",
      "metadata": {
        "id": "zCHqqqoxgW-2"
      },
      "outputs": [],
      "source": [
        "animeReduced = anime.drop(columns = ['name', 'score','type', 'episodes', 'popularity', 'members', 'favorites'])\n",
        "ratingsReduced = pd.DataFrame(columns = np.append(genres, 'user_id')).set_index('user_id')\n",
        "meanTable = pd.DataFrame(columns = np.append(genres, 'user_id')).set_index('user_id')\n",
        "\n",
        "users = len(ratings.user_id.unique().tolist())\n",
        "for i in range(6, users):\n",
        "  if users % i == 0:\n",
        "    break\n",
        "usersFolds = np.split(ratings.user_id.unique(), i)\n",
        "\n",
        "for i in usersFolds:\n",
        "  # Join each raring with related anime\n",
        "  genreTable = ratings[ratings.user_id.isin(i.tolist())].join(animeReduced, on ='anime_id').drop(columns = 'watching_status')\n",
        "  supportTable = ratings[ratings.user_id.isin(i.tolist())].drop(columns = ['watching_status', 'rating']).join(animeReduced, on = 'anime_id')\n",
        "  # Multiply presence on that genre with the rating given by user\n",
        "  genreTable[genres] = genreTable[genres].T.multiply(genreTable['rating']).T\n",
        "  supportTable = supportTable.drop(columns = 'anime_id').groupby(by = 'user_id').mean()\n",
        "  # Mean on genre for each user\n",
        "  genreTable.drop(columns = ['rating', 'anime_id'], inplace = True)\n",
        "  genreTable.replace(0, np.nan, inplace = True)\n",
        "  genreTable = genreTable.groupby(by = 'user_id').mean()\n",
        "  genreTable.replace(np.nan, 0, inplace = True)\n",
        "\n",
        "  ratingsReduced = ratingsReduced.append(genreTable, ignore_index = False)\n",
        "  meanTable = meanTable.append(supportTable, ignore_index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LslYeIeKqMVv",
      "metadata": {
        "id": "LslYeIeKqMVv"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del i\n",
        "del users\n",
        "del usersFolds\n",
        "del genreTable\n",
        "del animeReduced\n",
        "del supportTable\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NzvSAHdH4rWO",
      "metadata": {
        "id": "NzvSAHdH4rWO"
      },
      "source": [
        "This is the resulting table: each user has it's mean score for each genre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8JxHd5dZ4qgw",
      "metadata": {
        "id": "8JxHd5dZ4qgw"
      },
      "outputs": [],
      "source": [
        "ratingsReduced.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v5Myvuhv5JJp",
      "metadata": {
        "id": "v5Myvuhv5JJp"
      },
      "source": [
        "This is the table i will use to see the actual distribution of genres on each list: each value represent the presence of that genre in user list (ie. Action 0.37 for user 3 means that 37% of their list is of Action anime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iUzPlcJUanTy",
      "metadata": {
        "id": "iUzPlcJUanTy"
      },
      "outputs": [],
      "source": [
        "meanTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EYtVHom_uocT",
      "metadata": {
        "id": "EYtVHom_uocT"
      },
      "outputs": [],
      "source": [
        "def shuffleTrainTest(table = ratingsReduced, ttFactor = 0.6, verbose = True):\n",
        "\n",
        "  \"\"\"Shuffle unique users and split ratings in train and test set\n",
        "\n",
        "  Parameters:\n",
        "   > ttFactor (double) : default 0.6\n",
        "      How many users go in the training set (1 - ttFactor are in test set)\n",
        "\n",
        "  Returns:\n",
        "   > trainingSet, testSet\n",
        "  \n",
        "  \"\"\"\n",
        "  if((ttFactor > 0.8) or (ttFactor < 0)):\n",
        "    ttFactor = 0.6\n",
        "\n",
        "  uniqueUsers = table.index.unique().tolist()\n",
        "  rm.shuffle(uniqueUsers)\n",
        "\n",
        "  trainingUsers = []\n",
        "  testUsers = []\n",
        "\n",
        "  trainingSize = math.ceil(len(uniqueUsers) * ttFactor)\n",
        "  testSize = len(uniqueUsers) - trainingSize\n",
        "\n",
        "  for i in range(0,trainingSize):\n",
        "    trainingUsers.append(uniqueUsers[i])\n",
        "  for i in range(trainingSize, trainingSize + testSize):\n",
        "    testUsers.append(uniqueUsers[i])\n",
        "\n",
        "  trainingRatings = table[table.index.isin(trainingUsers)]\n",
        "  testRatings = table[table.index.isin(testUsers)]\n",
        "  if verbose:\n",
        "    print(f\"Training set: {len(trainingUsers)} users\\nTest set: {len(testUsers)} users\")\n",
        "  return trainingRatings, testRatings\n",
        "\n",
        "def scale(table):\n",
        "  return pd.DataFrame(\n",
        "      StandardScaler().fit_transform(table.T),\n",
        "      index = table.columns, columns = table.index).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pVcTewCNlKh1",
      "metadata": {
        "id": "pVcTewCNlKh1"
      },
      "source": [
        "## Avarage vote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yoixXE9p9iF4",
      "metadata": {
        "id": "yoixXE9p9iF4"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(4,4, figsize = (17,17))\n",
        "#----SOME MAJOR GENRES----\n",
        "sb.histplot(meanTable, x = 'Action', kde = True, stat = 'percent', ax = plot[0][0])\n",
        "sb.histplot(meanTable, x = 'Adventure', kde = True, stat = 'percent', ax = plot[0][1])\n",
        "sb.histplot(meanTable, x = 'Comedy', kde = True, stat = 'percent', ax = plot[0][2])\n",
        "sb.histplot(meanTable, x = 'Drama', kde = True, stat = 'percent', ax = plot[0][3])\n",
        "sb.histplot(meanTable, x = 'Fantasy', kde = True, stat = 'percent', ax = plot[1][0])\n",
        "sb.histplot(meanTable, x = 'Romance', kde = True, stat = 'percent', ax = plot[1][1])\n",
        "sb.histplot(meanTable, x = 'School', kde = True, stat = 'percent', ax = plot[1][2])\n",
        "sb.histplot(meanTable, x = 'Sci-Fi', kde = True, stat = 'percent', ax = plot[1][3])\n",
        "#----SOME MINOR GENRES----\n",
        "sb.histplot(meanTable, x = 'Demons', kde = True, stat = 'percent', ax = plot[2][0])\n",
        "sb.histplot(meanTable, x = 'Historical', kde = True, stat = 'percent', ax = plot[2][1])\n",
        "sb.histplot(meanTable, x = 'Horror', kde = True, stat = 'percent', ax = plot[2][2])\n",
        "sb.histplot(meanTable, x = 'Magic', kde = True, stat = 'percent', ax = plot[2][3])\n",
        "sb.histplot(meanTable, x = 'Mecha', kde = True, stat = 'percent', ax = plot[3][0])\n",
        "sb.histplot(meanTable, x = 'Music', kde = True, stat = 'percent', ax = plot[3][1])\n",
        "sb.histplot(meanTable, x = 'Parody', kde = True, stat = 'percent', ax = plot[3][2])\n",
        "sb.histplot(meanTable, x = 'Super Power', kde = True, stat = 'percent', ax = plot[3][3])\n",
        "fig.suptitle('Type presence in lists frequency distribution', fontsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zzepappL8TkT",
      "metadata": {
        "id": "zzepappL8TkT"
      },
      "source": [
        "In this table we can see the distribution of frequency of genres per user: for instance, 'Action' is present at 40% on avarage\n",
        "  - 2.5% of users, every 100 anime seen, 40 of those are Action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5cz--MBNdWg",
      "metadata": {
        "id": "b5cz--MBNdWg"
      },
      "source": [
        "# First step: Clustering\n",
        "\n",
        "In this step i will find clusters of users based on preferred genres.\n",
        "If i manage to find similiar users in this way, i can put any test users into a category to recommend with a more qualitative measure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aIwghgK-D0gg",
      "metadata": {
        "id": "aIwghgK-D0gg"
      },
      "source": [
        "Each user have a different method for voting: \n",
        "  - Some users keep their votes around 5~6\n",
        "  - Others prefer to give ratings around 8~9\n",
        "\n",
        "I will use the StandardScaler on rows to bring everyone to the same proportion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cSeQpYjYgu58",
      "metadata": {
        "id": "cSeQpYjYgu58"
      },
      "outputs": [],
      "source": [
        "trainingSet, testSet = shuffleTrainTest(ttFactor = 0.7)\n",
        "testJoin = scale(trainingSet)\n",
        "trainingSet2, testSet2 = shuffleTrainTest(table = meanTable, ttFactor = 0.7)\n",
        "testJoin.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3VaBfSa5_VLL",
      "metadata": {
        "id": "3VaBfSa5_VLL"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(figsize = (20,20))\n",
        "\n",
        "sb.heatmap(trainingSet2.corr(), linewidth=3, square = True, ax = plot, annot = True)\n",
        "plt.title('Correlation Matrix for genres presence', fontsize=16);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sJ3Qs_yYECM4",
      "metadata": {
        "id": "sJ3Qs_yYECM4"
      },
      "source": [
        "On the genres presence matrix there are some strongly correlated genres:\n",
        "  - Sci-Fi and Mecha has 0.87\n",
        "  - Mecha and Space has 0.87\n",
        "  - Mecha and Military has 0.82\n",
        "\n",
        "I can decide to remove two between Sci-Fi, Mecha and Space (for isntance the two with less occurrencies)\n",
        "\n",
        "Concerning Mecha and Military, i considered 0.82 not enough: i can decide anyway to choce Mecha in the first removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FQTTVJkCHzyP",
      "metadata": {
        "id": "FQTTVJkCHzyP"
      },
      "outputs": [],
      "source": [
        "anime[['Sci-Fi', 'Mecha', 'Space']].sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "-5SGSz3bIGf9",
      "metadata": {
        "id": "-5SGSz3bIGf9"
      },
      "source": [
        "I remove Space and Mecha columns becouse they have less occurrencies than Sci-Fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g7quA421Iy4l",
      "metadata": {
        "id": "g7quA421Iy4l"
      },
      "outputs": [],
      "source": [
        "genresSupp = genres\n",
        "genresSupp = genresSupp[genresSupp != 'Mecha']\n",
        "genresSupp = genresSupp[genresSupp != 'Space']\n",
        "# Remove anime with now no genre\n",
        "animeSupp = anime.loc[(anime.drop(columns = ['Mecha','Space'])[genresSupp].T.sum() > 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wDZXUP_dCHd4",
      "metadata": {
        "id": "wDZXUP_dCHd4"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(figsize = (20,20))\n",
        "\n",
        "sb.heatmap(testJoin.corr(), linewidth = 3, square = True, ax = plot, annot = True)\n",
        "plt.title('Correlation Matrix for scaled genres mean', fontsize=16);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Pg7q6jkhGXhO",
      "metadata": {
        "id": "Pg7q6jkhGXhO"
      },
      "source": [
        "Scaled genres mean dataset has no correlations between variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-zQ5O0_YD8Pf",
      "metadata": {
        "id": "-zQ5O0_YD8Pf"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(figsize = (20,20))\n",
        "\n",
        "sb.heatmap(trainingSet.corr(), linewidth = 3, square = True, ax = plot, annot = True)\n",
        "plt.title('Correlation Matrix for non-Scaled genres mean', fontsize=16);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7Q2Nqlv6HBE5",
      "metadata": {
        "id": "7Q2Nqlv6HBE5"
      },
      "source": [
        "On the other hand, not-scaled mean genres seem to have high correlation between most of them\n",
        "  - This is due to the fact that genres with high occurrencies (Action, Adventure, Comedy and so on) tends to have a mean score equal to the mean score of general dataset, that is from 6 to 8 as we saw with the boxplot on the score previously"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03vmnFN3-72l",
      "metadata": {
        "id": "03vmnFN3-72l"
      },
      "source": [
        "Let's see the hopkins statistics on scaled trainingSet and the genre presence dataset to see if it's better than the genre mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gk8glWOO42sW",
      "metadata": {
        "id": "gk8glWOO42sW"
      },
      "outputs": [],
      "source": [
        "x = int(testJoin.shape[0]*(1/10))\n",
        "y = int(trainingSet2.shape[0]*(1/10))\n",
        "print(f\"Scaled genres mean\\t\\t- {x}\\t samples: {1 - hopkins(testJoin,x)}\")\n",
        "print(f\"non-Scaled genres presence\\t- {x}\\t samples: {1 - hopkins(trainingSet2,y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rk4z7iJjPSnj",
      "metadata": {
        "id": "rk4z7iJjPSnj"
      },
      "source": [
        "Hopefully the datasets have high cluster tendency, becouse **Hopkins** score is higher than `0.75` (when Hopkins is 0.75 it is considered to have 90% cluster tendency)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aE40Uk6EHZZv",
      "metadata": {
        "id": "aE40Uk6EHZZv"
      },
      "source": [
        "I have to decide if to use the genres mean dataset or the genres presence dataset (removing highly correlated genres)\n",
        "\n",
        "I will test both with K-Means and choce the one with best trade-off between value of K and Silhouette score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IavRHGmGNszO",
      "metadata": {
        "id": "IavRHGmGNszO"
      },
      "outputs": [],
      "source": [
        "trainingSet2.drop(columns = ['Mecha', 'Space'], inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4J7Wl_cGzip-",
      "metadata": {
        "id": "4J7Wl_cGzip-"
      },
      "source": [
        "### KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7YN_Pgm-Pwnf",
      "metadata": {
        "id": "7YN_Pgm-Pwnf"
      },
      "source": [
        "As i am using K-means, i need to find the best K, so i'm gonna test both Elbow method and Silhouette score for different values of K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U_MS3iS1DBcm",
      "metadata": {
        "id": "U_MS3iS1DBcm"
      },
      "outputs": [],
      "source": [
        "silC = []\n",
        "silE = []\n",
        "elbow = []\n",
        "minK = 2\n",
        "maxK = 21\n",
        "\n",
        "for i in range(minK,maxK):\n",
        "  kmeans = KMeans(n_clusters = i, random_state = 77, n_init = 10)\n",
        "  y_kmeans = kmeans.fit_predict(testJoin)\n",
        "  silC.append(silhouette_score(testJoin, y_kmeans, metric = 'cosine'))\n",
        "  silE.append(silhouette_score(testJoin, y_kmeans, metric = 'euclidean'))\n",
        "  elbow.append(kmeans.inertia_)\n",
        "\n",
        "fig, plotSC = plt.subplots(2,1,figsize=(10,10))\n",
        "\n",
        "plotSC[0].plot(range(minK,maxK), silC, 'rx-')\n",
        "plotSC[0].set_ylabel('Silhouette Cosine score', color = 'r')\n",
        "plotE = plotSC[0].twinx()\n",
        "plotE.plot(range(minK,maxK), silE, 'bx-')\n",
        "plotE.set_ylabel('Silhouette Euclidean score', color = 'b')\n",
        "\n",
        "plotSC[1].plot(range(minK,maxK), elbow, 'gx-')\n",
        "plotSC[1].set_ylabel('Elbow method', color = 'g')\n",
        "\n",
        "plt.setp(plotSC, xticks=np.arange(minK,maxK,step=1))\n",
        "plotSC[0].set_xlabel('Value of K')\n",
        "plotSC[1].set_xlabel('Value of K')\n",
        "\n",
        "plt.title('Evaluation scores for K-Means', fontsize = 16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W1O-RPyd8JKH",
      "metadata": {
        "id": "W1O-RPyd8JKH"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "plot= plt.axes(projection = '3d')\n",
        "\n",
        "pca = PCA(n_components = 3).fit_transform(testJoin)\n",
        "clusters = KMeans(n_clusters = 12, random_state = 77, n_init = 10).fit_predict(testJoin)\n",
        "scatter = plot.scatter(pca[:,0], pca[:,1], pca[:,2], c = clusters, s=10, cmap='inferno_r')\n",
        "plt.legend(*scatter.legend_elements())\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClusters presence:\\n{pd.DataFrame(clusters).value_counts()}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Vg9LstDaMBJ1",
      "metadata": {
        "id": "Vg9LstDaMBJ1"
      },
      "source": [
        "Genre presence dataset have a really bad silhouette score, so i will reject it\n",
        "\n",
        "Results on scaled mean genres:\n",
        "  - As a first point of view, **Hopkins** sudgest that, with a score of `~0.85`, our dataset tends to have clusters\n",
        "\n",
        "  - **Elbow** is not very usefull in this dataset, since there isn't a well marked elbow point (the best seems to be 5)\n",
        "\n",
        "  - For **Silhouette score**, i have tested both Euclidean and Cosine distance: the sudgest an optimal `k = 4`. Cosine distance have a greater score compared to Euclidean, with a value of `~0.45`\n",
        "\n",
        "In the end i chosed `k = 12` becose the silhouette score is still high and i want more variety in clusters rather than only `k = 4`\n",
        "\n",
        "In any case 3D scatter plot scaled with PCA, seems to show some natural cluster, even if there is a huge disparity beween the biggest one and the others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vndOOqzQ6eKx",
      "metadata": {
        "id": "vndOOqzQ6eKx"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del clusters\n",
        "del fig\n",
        "del kmeans\n",
        "del elbow\n",
        "del maxK\n",
        "del minK\n",
        "del pca\n",
        "del plotSC\n",
        "del plotE\n",
        "del plot\n",
        "del silC\n",
        "del silE\n",
        "del scatter\n",
        "del genres\n",
        "del y_kmeans\n",
        "del x\n",
        "del i\n",
        "del y\n",
        "del animeSupp\n",
        "del genresSupp\n",
        "del testSet2\n",
        "del trainingSet2\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1XWwivG5znng",
      "metadata": {
        "id": "1XWwivG5znng"
      },
      "source": [
        "### Agglomerative clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jQWYc5PhtC98",
      "metadata": {
        "id": "jQWYc5PhtC98"
      },
      "outputs": [],
      "source": [
        "trainingSet, testSet = shuffleTrainTest(ttFactor = 0.3)\n",
        "testJoin = scale(trainingSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KYv8h24CjL0t",
      "metadata": {
        "id": "KYv8h24CjL0t"
      },
      "outputs": [],
      "source": [
        "def plot_dendrogram(model, **kwargs):\n",
        "    # Create linkage matrix and then plot the dendrogram\n",
        "\n",
        "    # create the counts of samples under each node\n",
        "    counts = np.zeros(model.children_.shape[0])\n",
        "    n_samples = len(model.labels_)\n",
        "    for i, merge in enumerate(model.children_):\n",
        "        current_count = 0\n",
        "        for child_idx in merge:\n",
        "            if child_idx < n_samples:\n",
        "                current_count += 1  # leaf node\n",
        "            else:\n",
        "                current_count += counts[child_idx - n_samples]\n",
        "        counts[i] = current_count\n",
        "\n",
        "    linkage_matrix = np.column_stack(\n",
        "        [model.children_, model.distances_, counts]\n",
        "    ).astype(float)\n",
        "\n",
        "    # Plot the corresponding dendrogram\n",
        "    dendrogram(linkage_matrix, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NoKbQ3OliRRp",
      "metadata": {
        "id": "NoKbQ3OliRRp"
      },
      "outputs": [],
      "source": [
        "k = 13 # Best selected K according to K-Means \n",
        "\n",
        "for l in ['single','complete','average','ward']:\n",
        "    fig, plot = plt.subplots(figsize=(15,15))\n",
        "    agnes = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage = l)\n",
        "    agnes = agnes.fit(testJoin)\n",
        "    plt.title(f\"Hierarchical Clustering Dendrogram - {l} linkage\")\n",
        "    # plot the top three levels of the dendrogram\n",
        "    plot_dendrogram(agnes, truncate_mode = \"level\", p = 3)\n",
        "    plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
        "    plt.show()\n",
        "\n",
        "    agnes = AgglomerativeClustering(n_clusters=k, linkage = l)\n",
        "    y_agnes = agnes.fit_predict(testJoin)\n",
        "    a = silhouette_score(testJoin, y_agnes, metric = 'cosine')\n",
        "    b = silhouette_score(testJoin, y_agnes, metric = 'euclidean')\n",
        "    print(f\"{l} linkage - k = {k} - Cosine Silhouette = {a}, Euclidean Silhouette = {b}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SoFnMTK-GOyK",
      "metadata": {
        "id": "SoFnMTK-GOyK"
      },
      "source": [
        "I calculated some dendograms for all linkage types along with Silhouette score for n_clusters = 13, since it is the best k i selected with K-Means\n",
        "\n",
        "I will calculate the best K later when chosed the best linkage method for this dataset:\n",
        "  - **Single linkage**: just create 1 big cluster and leave less than 10 samples alone with their own cluster - `Rejected`\n",
        "  - **Avarage linkage**: even if the Silhouette score is pretty decent, it creates the same disparity that Single linkage does - `Rejected`\n",
        "\n",
        "**Ward linkage** seems to be the best as it provides both a decent Silhouette score and distribution of points into clusters\n",
        "\n",
        "I will give to **Complete linkage** a chance anyway and just test both them for some values of K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qHGEYXvMCvhZ",
      "metadata": {
        "id": "qHGEYXvMCvhZ"
      },
      "outputs": [],
      "source": [
        "# I do it in a small portion of the whole dataset or it will takes hours\n",
        "# Unfortunatley the resoult will not be very relyable\n",
        "trainingSet, testSet = shuffleTrainTest(ttFactor = 0.4)\n",
        "testJoin = scale(trainingSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8LiVLIDIu_X",
      "metadata": {
        "id": "d8LiVLIDIu_X"
      },
      "outputs": [],
      "source": [
        "minRange = 2\n",
        "maxRange = 21\n",
        "\n",
        "silCC = []\n",
        "silCE = []\n",
        "silWC = []\n",
        "silWE = []\n",
        "\n",
        "for l in ['ward', 'complete']:\n",
        "  for n in range(minRange,maxRange):\n",
        "    y_agnes = AgglomerativeClustering(n_clusters = n, linkage = l).fit_predict(testJoin)\n",
        "    a = silhouette_score(testJoin, y_agnes, metric = 'cosine')\n",
        "    b = silhouette_score(testJoin, y_agnes, metric = 'euclidean')\n",
        "    if l == 'ward':\n",
        "      silWC.append(a) # Ward Cosine\n",
        "      silWE.append(b) # Ward Euclidean\n",
        "    else:\n",
        "      silCC.append(a) # Complete Cosine\n",
        "      silCE.append(b) # Complete Euclidean\n",
        "\n",
        "\n",
        "fig, plot = plt.subplots(2,1,figsize=(10,10))\n",
        "fig.suptitle('Evaluation scores for AGNES', fontsize = 16)\n",
        "\n",
        "plt.setp(plot, xticks=np.arange(minRange,maxRange,step=1))\n",
        "plot[0].plot(range(minRange,maxRange), silWC, 'rx-')\n",
        "plot[0].set_ylabel('Cosine score', color = 'r')\n",
        "plot[0].set_title('Ward linkage')\n",
        "plotW = plot[0].twinx()\n",
        "plotW.plot(range(minRange,maxRange), silWE, 'bx-')\n",
        "plotW.set_ylabel('Euclidean score', color = 'b')\n",
        "\n",
        "plot[1].plot(range(minRange,maxRange), silCC, 'rx-')\n",
        "plot[1].set_ylabel('Cosine score', color = 'r')\n",
        "plot[1].set_title('Complete linkage')\n",
        "plotC = plot[1].twinx()\n",
        "plotC.plot(range(minRange,maxRange), silCE, 'bx-')\n",
        "plotC.set_ylabel('Euclidean score', color = 'b')\n",
        "\n",
        "plot[0].set_xlabel('Value of K')\n",
        "plot[1].set_xlabel('Value of K')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aWyDsbFGMF_3",
      "metadata": {
        "id": "aWyDsbFGMF_3"
      },
      "source": [
        "They works good in different range of K:\n",
        "  - **Both** seem to perform better with `K = 10-15` and then they have a drop\n",
        "  - Despite this, **Ward linkage** has a better result in Silhouette score, so i will chose it between the two\n",
        "\n",
        "I will also chose K = 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GoiF0T1KrAkC",
      "metadata": {
        "id": "GoiF0T1KrAkC"
      },
      "outputs": [],
      "source": [
        "trainingSet, testSet = shuffleTrainTest(ttFactor = 0.4)\n",
        "testJoin = scale(trainingSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-GwkkAxjQwBe",
      "metadata": {
        "id": "-GwkkAxjQwBe"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "plot= plt.axes(projection = '3d')\n",
        "\n",
        "pca = PCA(n_components = 3).fit_transform(testJoin)\n",
        "clusters = AgglomerativeClustering(n_clusters = 14, linkage = 'ward').fit_predict(testJoin)\n",
        "scatter = plot.scatter(pca[:,0], pca[:,1], pca[:,2], c = clusters, s=10, cmap='inferno') # since 0 are 'others' not in a cluster, invert colors\n",
        "plt.legend(*scatter.legend_elements())\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClusters presence:\\n{pd.DataFrame(clusters).value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M5bxsvIRfez7",
      "metadata": {
        "id": "M5bxsvIRfez7"
      },
      "source": [
        "As result, **Agglomerative Clustering** with ward linkage works pretty decent, giving us resoults similiar to **K-Means**, both in cluster quality and cluster balance\n",
        "\n",
        "Anyway, K-Means it's faster with more accuracy and clusters balance, so i will use it as clustering method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uPDLxMyTlGpo",
      "metadata": {
        "id": "uPDLxMyTlGpo"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del a\n",
        "del agnes\n",
        "del b\n",
        "del fig\n",
        "del k\n",
        "del l\n",
        "del maxRange\n",
        "del minRange\n",
        "del n\n",
        "del plot\n",
        "del silCE\n",
        "del silCC\n",
        "del silWE\n",
        "del silWC\n",
        "del y_agnes\n",
        "del scatter\n",
        "del plotC\n",
        "del plotW\n",
        "del pca\n",
        "del clusters\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XbQJA561jowW",
      "metadata": {
        "id": "XbQJA561jowW"
      },
      "source": [
        "### DBSCAN\n",
        "\n",
        "I will also give DBSCAN a chance to check if the dataset it's made by non-convex shaped clusters\n",
        "\n",
        "First of all i will use OPTICS with differents MinPts to find the best Eps for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aUXE7YxDsqi7",
      "metadata": {
        "id": "aUXE7YxDsqi7"
      },
      "outputs": [],
      "source": [
        "np.seterr(divide='ignore', invalid='ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "xE6FYD7o-G_g",
      "metadata": {
        "id": "xE6FYD7o-G_g"
      },
      "source": [
        "I will run **OPTICS** with minPts = 5 on the trainingSet to get a value for Eps and see if there are natural clusters with same density inside the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MbLYSIKMGapV",
      "metadata": {
        "id": "MbLYSIKMGapV"
      },
      "outputs": [],
      "source": [
        "trainingSet, testSet = shuffleTrainTest(ttFactor = 0.6)\n",
        "trainingSet2, testSet2 = shuffleTrainTest(meanTable, ttFactor = 0.2, verbose = False)\n",
        "testJoin = scale(trainingSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3E0_OncKCpY8",
      "metadata": {
        "id": "3E0_OncKCpY8"
      },
      "outputs": [],
      "source": [
        "fig, plot = plt.subplots(1,1,figsize=(30,20))\n",
        "\n",
        "optic = OPTICS(min_samples = 5, metric = 'euclidean')\n",
        "optic.fit(testJoin)\n",
        "plot.plot(np.arange(testJoin.shape[0]), (optic.reachability_[optic.ordering_]), c = 'k')\n",
        "\n",
        "# plt.ylim((0,3))\n",
        "# plt.setp(plot, yticks=np.arange(0,0.30,0.01))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oR4else0E4eW",
      "metadata": {
        "id": "oR4else0E4eW"
      },
      "source": [
        "**OPTICS** shows that there are some natural clusters, despite the huge difference in density. We can se that there are more or less 6 major natural clusters with a Eps = 1, while the rest of points seems to be very far away from the rest\n",
        "\n",
        "\n",
        "I will give DBSCAN a try with with minPts = 5 and Eps = 1 and see what i get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4UpFqjq2_vjT",
      "metadata": {
        "id": "4UpFqjq2_vjT"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "fig = plt.figure(figsize = (15,15))\n",
        "plot= plt.axes(projection = '3d')\n",
        "\n",
        "pca = PCA(n_components = 3).fit_transform(testJoin)\n",
        "clusters = DBSCAN(eps = 1, min_samples = 5, metric='euclidean').fit_predict(testJoin)\n",
        "scatter = plot.scatter(pca[:,0], pca[:,1], pca[:,2], c = clusters, s=2, cmap='inferno_r') # since 0 are 'others' not in a cluster, invert colors\n",
        "plt.legend(*scatter.legend_elements())\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClusters presence:\\n{pd.DataFrame(clusters).value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QTfPMDotFcI0",
      "metadata": {
        "id": "QTfPMDotFcI0"
      },
      "source": [
        "As expected, the a lot of the points (those in cluster -1) are considered outliers becouse of the different densities shown by OPTICS\n",
        "\n",
        "Anyway, it is shown that the majority of the points is in the first 1-2 clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hcxJLcbJlhv8",
      "metadata": {
        "id": "hcxJLcbJlhv8"
      },
      "outputs": [],
      "source": [
        "trainingSet, testSet = shuffleTrainTest(ttFactor = 0.7)\n",
        "testJoin = scale(trainingSet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ALkStDwxmTFY",
      "metadata": {
        "id": "ALkStDwxmTFY"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del optic\n",
        "del fig\n",
        "del plot\n",
        "del clusters\n",
        "del pca\n",
        "del scatter\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ngToK4b2G7U_",
      "metadata": {
        "id": "ngToK4b2G7U_"
      },
      "source": [
        "## Ending results\n",
        "*Runtime calculated on 70% of the users for K-Means and DBSCAN, 30% for Agglomerative Clustering*\n",
        "\n",
        "**K-Means**\n",
        "  - `K`: 12\n",
        "  - `Silhouette score`: 0.45 (Cosine distance)\n",
        "  - `Runtime in seconds`: 4.1s\n",
        "\n",
        "**Agglomerative clustering**\n",
        "  - `K`: 14\n",
        "  - `Silhouette score`: 0.375 (Cosine distance)\n",
        "  - `Runtime in seconds`: 44.4s\n",
        "\n",
        "**DBSCAN**\n",
        "  - `Eps`: 1\n",
        "  - `Minpts`: 5\n",
        "  - `Clusters found`: 6 with reasonable number of members\n",
        "  - `Outliers`: More than 50% of the points\n",
        "  - `Runtime in seconds`: 14.7s\n",
        "\n",
        "\n",
        "  K-Means is the best algorithm found for this dataset, it offers a well marked trade-off between clusters silhouette, clusters variety and runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RPQdc5XeVsuO",
      "metadata": {
        "id": "RPQdc5XeVsuO"
      },
      "outputs": [],
      "source": [
        "#----------------CHECKPOINT---------------\n",
        "anime.to_csv(root + '/animeCheckpoint2.csv')\n",
        "ratings.to_csv(root + '/ratingsCheckpoint2.csv')\n",
        "ratingsReduced.to_csv(root + '/ratingsReducedCheckpoint2.csv')\n",
        "#-----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ugXZAf3WVgxV",
      "metadata": {
        "id": "ugXZAf3WVgxV"
      },
      "source": [
        "---------------------------------------\n",
        "AT THIS POINT ITS BETTER IF YOU RESTART THE RUNTIME AND START OVER WITH CHEKPOINT DATASETS (PART 2)\n",
        "\n",
        "---------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kYwWCnj9Vn_9",
      "metadata": {
        "id": "kYwWCnj9Vn_9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 50)\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import random as rm\n",
        "import math\n",
        "import gc\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "root = './Datasets'\n",
        "anime = pd.read_csv(root + '/animeCheckpoint2.csv', index_col='anime_id')\n",
        "ratings = pd.read_csv(root + '/ratingsCheckpoint2.csv').drop(columns = 'watching_status')\n",
        "ratingsReduced = pd.read_csv(root + '/ratingsReducedCheckpoint2.csv', index_col = 'user_id')\n",
        "\n",
        "ratings.drop(columns = ['Unnamed: 0'], inplace = True)\n",
        "# {1: 'Watching', 2: 'Completed', 3: 'On-Hold', 4: 'Dropped', 6: 'Planning'}\n",
        "genres = anime.drop(columns = ['episodes', 'type','name','score','popularity','members','favorites']).columns.to_numpy()\n",
        "animeReduced = anime.drop(columns = ['name','score','type','episodes','members','favorites','popularity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3mpu6s5CWLcU",
      "metadata": {
        "id": "3mpu6s5CWLcU"
      },
      "outputs": [],
      "source": [
        "def shuffleTrainTest(table = ratingsReduced, ttFactor = 0.6, verbose = True):\n",
        "\n",
        "  \"\"\"Shuffle unique users and split ratings in train and test set\n",
        "\n",
        "  Parameters:\n",
        "   > ttFactor (double) : default 0.6\n",
        "      How many users go in the training set (1 - ttFactor are in test set)\n",
        "\n",
        "  Returns:\n",
        "   > trainingSet, testSet\n",
        "  \n",
        "  \"\"\"\n",
        "  if((ttFactor > 0.8) or (ttFactor < 0)):\n",
        "    ttFactor = 0.6\n",
        "\n",
        "  uniqueUsers = table.index.unique().tolist()\n",
        "  rm.shuffle(uniqueUsers)\n",
        "\n",
        "  trainingUsers = []\n",
        "  testUsers = []\n",
        "\n",
        "  trainingSize = math.ceil(len(uniqueUsers) * ttFactor)\n",
        "  testSize = len(uniqueUsers) - trainingSize\n",
        "\n",
        "  for i in range(0,trainingSize):\n",
        "    trainingUsers.append(uniqueUsers[i])\n",
        "  for i in range(trainingSize, trainingSize + testSize):\n",
        "    testUsers.append(uniqueUsers[i])\n",
        "\n",
        "  trainingRatings = table[table.index.isin(trainingUsers)]\n",
        "  testRatings = table[table.index.isin(testUsers)]\n",
        "  if verbose:\n",
        "    print(f\"Training set: {len(trainingUsers)} users\\nTest set: {len(testUsers)} users\")\n",
        "    \n",
        "  return trainingRatings, testRatings\n",
        "\n",
        "def scale(table):\n",
        "  \"\"\"Scale the given table using StandardScaler\n",
        "    on the rows\n",
        "  \"\"\"\n",
        "  return pd.DataFrame(\n",
        "      StandardScaler().fit_transform(table.T),\n",
        "      index = table.columns, columns = table.index).T\n",
        "\n",
        "def mindiv(n, startn = 2):\n",
        "  for i in range(startn, n):\n",
        "    if n%i==0:\n",
        "      return i"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9FKsifLPTuHm",
      "metadata": {
        "id": "9FKsifLPTuHm"
      },
      "source": [
        "# Second step: Collaborative-filtering\n",
        "\n",
        "Once i have the clusters of similiar users, i will give to the test user a recommendation, trying two methods:\n",
        "   - First, i will take the list of anime of users in the same cluster,\n",
        "   remove those already seen by the user under test, and then sort by occurrence: an anime seen by 6 users in this cluster is higher in the list rather than one seen by 4 users. In case of equality, i will sort by popularity in general\n",
        "\n",
        "   - Second, i will try to implement a recommendation system like those used by major Companies like Amazon or Netflix. It uses a table like the one below to calculate the best item to see next, based on the best K users similiar to me\n",
        "\n",
        "Example:\n",
        "```\n",
        "        | Show X | Show Y | Show Z |\n",
        "--------+--------+--------+--------+\n",
        "User A  |    0   |    5   |   8    |\n",
        "--------+--------+--------+--------+\n",
        "User B  |    7   |    5   |   2    |\n",
        "--------+--------+--------+--------+\n",
        "User C  |    8   |    0   |   0    |\n",
        "--------+--------+--------+--------+\n",
        "```\n",
        "\n",
        "Users B and C may be more similiar each others than A and B or A and C\n",
        " - I can suggest to B something that C already saw, but B didn't"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efn4pYaOgBFY",
      "metadata": {
        "id": "efn4pYaOgBFY"
      },
      "source": [
        "For the moment, train the KMeans and create a table where each `user_id` in the trainingSet has it's own cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "msN2UuJwc3jK",
      "metadata": {
        "id": "msN2UuJwc3jK"
      },
      "outputs": [],
      "source": [
        "cumulativePrecision = 0\n",
        "maxRounds = 10\n",
        "folds = np.array_split(ratingsReduced.sample(frac=1, random_state = 619), 10)\n",
        "\n",
        "for t in range(maxRounds):\n",
        "  cumulativeHit = 0\n",
        "  cumulativeCount = 0\n",
        "  \n",
        "  trainingSet = pd.DataFrame(columns = np.append(ratingsReduced.columns, 'user_id')).set_index('user_id')\n",
        "  testSet = pd.DataFrame(columns = np.append(ratingsReduced.columns, 'user_id')).set_index('user_id')\n",
        "\n",
        "  for j in range(maxRounds):\n",
        "    if (j == t):\n",
        "      testSet = testSet.append(folds[j])\n",
        "    else:\n",
        "      trainingSet = trainingSet.append(folds[j])\n",
        "\n",
        "  print(f\"Fold {t+1} of {maxRounds}\")\n",
        "\n",
        "  # Scale the training set\n",
        "  scaledTrainingSet = scale(trainingSet)\n",
        "  # Build clusters\n",
        "  kmeans = KMeans(n_clusters = 13, random_state = 523, n_init = 10).fit(scaledTrainingSet)\n",
        "  # Take the cluster for each training sample\n",
        "  y_kmeans = kmeans.predict(scaledTrainingSet)\n",
        "  # Create a table with user_id-cluster pairs\n",
        "  clusters = pd.DataFrame(y_kmeans, columns = ['cluster'], index = scaledTrainingSet.index)\n",
        "  # Take id of test users\n",
        "  testBase = testSet.index.tolist()\n",
        "  # Shuffle them\n",
        "  rm.shuffle(testBase)\n",
        "  # Test this round on first 50 random\n",
        "  for x in testBase[:150]:\n",
        "    # Retrieve ratings of user under test\n",
        "    userList = ratings[ratings.user_id == x].anime_id.unique().tolist()\n",
        "    # Randomly sample 3/5 of those to associate the user under test to a cluster\n",
        "    sampledList = rm.sample(userList, int(len(userList)*3/5))\n",
        "    testList = []\n",
        "    # The remaining 2/5 will be used as test of precision\n",
        "    for i in userList:\n",
        "      if i not in sampledList:\n",
        "        testList.append(i)\n",
        "    # Retrieve samped ratings of the user under test, join with anime, multiply genre presence by rating and mean on user_id\n",
        "    temp = ratings.loc[(ratings['user_id'] == x) & (ratings.anime_id.isin(sampledList))].join(animeReduced, on = 'anime_id').drop(columns = ['anime_id'])\n",
        "    temp[genres] = temp[genres].T.multiply(temp['rating']).T\n",
        "    temp = temp.drop(columns = 'rating').replace(0,np.nan).groupby(by = 'user_id').mean().replace(np.nan,0)\n",
        "    # Also scale the user rating list\n",
        "    scaledTest = scale(temp)\n",
        "    # Cluster label for tested testUser\n",
        "    label = kmeans.predict(scaledTest)\n",
        "    # Inizialize the KNN algorithm for 15 users\n",
        "    knn = NearestNeighbors(n_neighbors = 15, metric = 'cosine', algorithm='brute')\n",
        "    # Fit the model for training users in the same cluster of user under test\n",
        "    knn.fit(scaledTrainingSet[scaledTrainingSet.index.isin(clusters[clusters.cluster == label[0]].index)])\n",
        "    # Retreive 15 NearestNeighbors\n",
        "    indexes = knn.kneighbors(scaledTest, return_distance = False)\n",
        "    # Pick KNN ratings list (group by anime_id to remove duplicates)\n",
        "    neighborsList = ratings[ratings.user_id.isin(indexes[0])].groupby(by = 'anime_id').mean().drop(columns = ['user_id'])\n",
        "    # Now remove those already present in the sampled list of the user under test\n",
        "    recommandationList = neighborsList[~neighborsList.index.isin(sampledList)].rename(columns = {'rating': 'vote'})\n",
        "    # Only use the topTen anime sorted by members\n",
        "    topTen = recommandationList.join(anime, on = 'anime_id').sort_values(by = ['members'], ascending = False).head(10)\n",
        "    # Precision is HOW MANY OF THOSE 10 ARE PRESENT IN THE REMAINING 2/5 divided by 10\n",
        "    cumulativeHit = cumulativeHit + topTen[topTen.index.isin(testList)].shape[0]/max(1, topTen.shape[0])\n",
        "    cumulativeCount = cumulativeCount + 1\n",
        "  print(f\"\\tStage accuracy: {int((cumulativeHit/cumulativeCount)*100)}%\\n\")\n",
        "  cumulativePrecision += int((cumulativeHit/cumulativeCount)*100)\n",
        "print(f\"\\nOverall accuracy: {int(cumulativePrecision / 10)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pClqJMQfqwkW",
      "metadata": {
        "id": "pClqJMQfqwkW"
      },
      "source": [
        "`50+%` overall accuracy means that if we give a user a list of 10 recommanded anime, at least 5 are interesting to them\n",
        "  - This score is calculated by removing some anime from someone's list to see if the system will recommend any of the remaining to the user\n",
        "---\n",
        "---\n",
        "I compared this score with other methods i tried (not in this notebook), and they had an overall score of 30-40%, so this result is not that bad\n",
        "\n",
        "I will try anyway another method i found in some paper: this is the old method Netflix.com used some years ago to recommend films to users\n",
        "  - I will anyway use the clustering method used before to pick up similiar users (it will have a role of numerosity reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G6zEFFrQsuyF",
      "metadata": {
        "id": "G6zEFFrQsuyF"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del clusters\n",
        "del cumulativeCount\n",
        "del cumulativeHit\n",
        "del indexes\n",
        "del kmeans\n",
        "del knn\n",
        "del label\n",
        "del maxRounds\n",
        "del neighborsList\n",
        "del recommandationList\n",
        "del sampledList\n",
        "del scaledTest\n",
        "del scaledTrainingSet\n",
        "del t\n",
        "del temp\n",
        "del testBase\n",
        "del testList\n",
        "del testSet\n",
        "del topTen\n",
        "del trainingSet\n",
        "del userList\n",
        "del x\n",
        "del i\n",
        "del y_kmeans\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FuMiIWS53wca",
      "metadata": {
        "id": "FuMiIWS53wca"
      },
      "outputs": [],
      "source": [
        "cumulativePrecision = 0\n",
        "maxRounds = 10\n",
        "folds = np.array_split(ratingsReduced, 10)\n",
        "\n",
        "for t in range(maxRounds):\n",
        "  cumulativeHit = 0\n",
        "  cumulativeCount = 0\n",
        "  \n",
        "  trainingSet = pd.DataFrame(columns = np.append(ratingsReduced.columns, 'user_id')).set_index('user_id')\n",
        "  testSet = pd.DataFrame(columns = np.append(ratingsReduced.columns, 'user_id')).set_index('user_id')\n",
        "\n",
        "  for j in range(maxRounds):\n",
        "    if (j == t):\n",
        "      testSet = testSet.append(folds[j])\n",
        "    else:\n",
        "      trainingSet = trainingSet.append(folds[j])\n",
        "\n",
        "  print(f\"Fold {t+1} of {maxRounds}\")\n",
        "\n",
        "  # Scale the training set\n",
        "  scaledTrainingSet = scale(trainingSet)\n",
        "  # Build clusters\n",
        "  kmeans = KMeans(n_clusters = 13, random_state = 523, n_init = 10).fit(scaledTrainingSet)\n",
        "  # Take the cluster for each training sample\n",
        "  y_kmeans = kmeans.predict(scaledTrainingSet)\n",
        "  # Create a table with user_id-cluster pairs\n",
        "  clusters = pd.DataFrame(y_kmeans, columns = ['cluster'], index = scaledTrainingSet.index)\n",
        "  # Take id of test users\n",
        "  testBase = testSet.index.tolist()\n",
        "  # Shuffle them\n",
        "  rm.shuffle(testBase)\n",
        "  # Test this round on first 20 random\n",
        "  for x in testBase[:50]:\n",
        "    # Retrieve ratings of user under test\n",
        "    userList = ratings[ratings.user_id == x].anime_id.unique().tolist()\n",
        "    # Randomly sample 3/5 of those to associate the user under test to a cluster\n",
        "    sampledList = rm.sample(userList, int(len(userList)*3/5))\n",
        "    testList = []\n",
        "    # The remaining 2/5 will be used as test of precision\n",
        "    for i in userList:\n",
        "      if i not in sampledList:\n",
        "        testList.append(i)\n",
        "    # Retrieve samped ratings of the user under test, join with anime, multiply genre presence by rating and mean on user_id\n",
        "    temp = ratings.loc[(ratings['user_id'] == x) & (ratings.anime_id.isin(sampledList))].join(animeReduced, on = 'anime_id').drop(columns = ['anime_id'])\n",
        "    temp[genres] = temp[genres].T.multiply(temp['rating']).T\n",
        "    temp = temp.drop(columns = 'rating').replace(0,np.nan).groupby(by = 'user_id').mean().replace(np.nan,0)\n",
        "    # Also scale the user rating list\n",
        "    scaledTest = scale(temp)\n",
        "    # Cluster label for tested testUser\n",
        "    label = kmeans.predict(scaledTest)\n",
        "    # Pick similiar users\n",
        "    similarUsers = clusters[clusters['cluster'] == label[0]].index\n",
        "    # Ratings of sampled user and test and all users in same cluster\n",
        "    knnTable = ratings.loc[((ratings.user_id == x)&(ratings.anime_id.isin(sampledList)))|(ratings.user_id.isin(similarUsers))]\n",
        "    # Build the similarity matrix\n",
        "    similarityMatrix = knnTable.pivot(index = 'user_id', columns = 'anime_id', values = 'rating').fillna(0).astype(np.uint8)\n",
        "    # Inizialize the KNN algorithm for 15 users\n",
        "    knn = NearestNeighbors(n_neighbors = 15, metric = 'euclidean', algorithm = 'brute')\n",
        "    # Fit the model for training users in the same cluster of user under test\n",
        "    knn.fit(similarityMatrix.loc[~(similarityMatrix.index == x)])\n",
        "    # Retreive 15 NearestNeighbors\n",
        "    indexes = knn.kneighbors(similarityMatrix.loc[(similarityMatrix.index == x)], return_distance = False)\n",
        "    # same as in previous method, but using indexes from nearest users\n",
        "    neighborsList = ratings[ratings.user_id.isin(indexes[0])].groupby(by = 'anime_id').mean().drop(columns = ['user_id'])\n",
        "    # Now remove those already present in the sampled list of the user under test\n",
        "    recommandationList = neighborsList[~neighborsList.index.isin(sampledList)].rename(columns = {'rating': 'vote'})\n",
        "    # Only use the topTen anime sorted by members\n",
        "    topTen = recommandationList.join(anime, on = 'anime_id').sort_values(by = ['members'], ascending = False).head(10)\n",
        "    # Precision is HOW MANY OF THOSE 10 ARE PRESENT IN THE REMAINING 2/5 divided by 10\n",
        "    cumulativeHit = cumulativeHit + topTen[topTen.index.isin(testList)].shape[0]/max(1, topTen.shape[0])\n",
        "    cumulativeCount = cumulativeCount + 1\n",
        "\n",
        "  print(f\"\\tStage accuracy: {int((cumulativeHit/cumulativeCount)*100)}%\\n\")\n",
        "  cumulativePrecision += int((cumulativeHit/cumulativeCount)*100)\n",
        "print(f\"\\nOverall accuracy: {int(cumulativePrecision / 10)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3H_0U-tmctqm",
      "metadata": {
        "id": "3H_0U-tmctqm"
      },
      "outputs": [],
      "source": [
        "#-------------MEMORY CLEANING-------------\n",
        "del clusters\n",
        "del cumulativeCount\n",
        "del cumulativeHit\n",
        "del indexes\n",
        "del kmeans\n",
        "del knn\n",
        "del label\n",
        "del maxRounds\n",
        "del neighborsList\n",
        "del recommandationList\n",
        "del sampledList\n",
        "del scaledTest\n",
        "del scaledTrainingSet\n",
        "del t\n",
        "del temp\n",
        "del testBase\n",
        "del testList\n",
        "del testSet\n",
        "del topTen\n",
        "del trainingSet\n",
        "del userList\n",
        "del x\n",
        "del i\n",
        "del y_kmeans\n",
        "\n",
        "gc.collect()\n",
        "#-------------MEMORY CLEANING-------------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "71fFn5hBi8EN",
      "metadata": {
        "id": "71fFn5hBi8EN"
      },
      "source": [
        "The following is a Pure User-Based Collaborative Filtering\n",
        "\n",
        "I have implemented it as explained in the licterature without using the clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BqcqtIMxi5Re",
      "metadata": {
        "id": "BqcqtIMxi5Re"
      },
      "outputs": [],
      "source": [
        "cumulativePrecision = 0\n",
        "maxRounds = 10\n",
        "folds = np.array_split(ratingsReduced, 10)\n",
        "\n",
        "for t in range(maxRounds):\n",
        "  cumulativeHit = 0\n",
        "  cumulativeCount = 0\n",
        "  \n",
        "  trainingSet = pd.DataFrame(columns = np.append(ratingsReduced.columns, 'user_id')).set_index('user_id')\n",
        "  testSet = pd.DataFrame(columns = np.append(ratingsReduced.columns, 'user_id')).set_index('user_id')\n",
        "\n",
        "  for j in range(maxRounds):\n",
        "    if (j == t):\n",
        "      testSet = testSet.append(folds[j])\n",
        "    else:\n",
        "      trainingSet = trainingSet.append(folds[j])\n",
        "\n",
        "  print(f\"Fold {t+1} of {maxRounds}\")\n",
        "  # Take id of training users\n",
        "  trainingBase = trainingSet.index.tolist()\n",
        "  # Take id of test users\n",
        "  testBase = testSet.index.tolist()\n",
        "  # Shuffle test users\n",
        "  rm.shuffle(testBase)\n",
        "  # Test this round on first 20 random\n",
        "  for x in testBase[:50]:\n",
        "    # Retrieve ratings of user under test\n",
        "    userList = ratings[ratings.user_id == x].anime_id.unique().tolist()\n",
        "    # Randomly sample 3/5 of those to associate the user under test to a cluster\n",
        "    sampledList = rm.sample(userList, int(len(userList)*3/5))\n",
        "    testList = []\n",
        "    # The remaining 2/5 will be used as test of precision\n",
        "    for i in userList:\n",
        "      if i not in sampledList:\n",
        "        testList.append(i)\n",
        "    # Inizialize vectors to find similiar users in chunk\n",
        "    dist = []\n",
        "    indx = []\n",
        "    # For all user in each chunk\n",
        "    for chunk in np.split(np.array(trainingBase), mindiv(len(trainingBase))):\n",
        "      # Retreive all ratings for those users and user under test but only their sampled list\n",
        "      knnTable = ratings.loc[((ratings.user_id == x) & (ratings.anime_id.isin(sampledList))) | (ratings.user_id.isin(chunk))]\n",
        "      # Build the similarity matrix (aka pivot table)\n",
        "      similarityMatrix = knnTable.pivot(index = 'user_id', columns = 'anime_id', values = 'rating').fillna(0).astype(np.uint8)\n",
        "      # Inizialize Knn and find 5 neighbors for each chunk\n",
        "      knn = NearestNeighbors(n_neighbors = 5, metric = 'euclidean', algorithm = 'brute').fit(similarityMatrix.loc[~(similarityMatrix.index == x)])\n",
        "      distances, indexes = knn.kneighbors(similarityMatrix.loc[(similarityMatrix.index == x)])\n",
        "      dist.append(distances)\n",
        "      indx.append(indexes)\n",
        "    # This is the distance table for all chunks, ordered by distance\n",
        "    distanceTab = pd.DataFrame(data = np.reshape(dist, 35), index = np.reshape(indx, 35), columns = ['distance']).sort_values(by = 'distance', ascending = True)\n",
        "    # Get first 5 users list\n",
        "    nearestUsers = distanceTab.head(10).index.tolist()\n",
        "    # Get their rating list\n",
        "    neighborsList = ratings[ratings.user_id.isin(nearestUsers)].groupby(by = 'anime_id').mean().drop(columns = ['user_id'])\n",
        "    # recommandation list without already seen anime\n",
        "    recommandationList = neighborsList.loc[~(neighborsList.index.isin(sampledList))].rename(columns = {'rating': 'vote'}).join(anime, on = 'anime_id').drop(columns = genres)\n",
        "    # topTen recommended anime\n",
        "    topTen = recommandationList.sort_values(by = ['members'], ascending = False).head(10)\n",
        "    # Precision is HOW MANY OF THOSE 10 ARE PRESENT IN THE REMAINING 2/5 divided by 10\n",
        "    cumulativeHit = cumulativeHit + topTen[topTen.index.isin(testList)].shape[0]/max(1, topTen.shape[0])\n",
        "    cumulativeCount = cumulativeCount + 1\n",
        "\n",
        "  print(f\"\\tStage accuracy: {int((cumulativeHit/cumulativeCount)*100)}%\\n\")\n",
        "  cumulativePrecision += int((cumulativeHit/cumulativeCount)*100)\n",
        "print(f\"\\nOverall accuracy: {int(cumulativePrecision / 10)}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAO3714fPghS",
      "metadata": {
        "id": "xAO3714fPghS"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Trying to predict what an user may like it's an extremly difficoult task: leaving aside the fact that we are not rational beings, every human is different from another and trying to suggest something from similar users it's not very accurate\n",
        "\n",
        "In this project i tried to give every user a profile, based on the mean score they gave to each genre. Becouse of the high dimensionality and numerosity, i had to reduce in size every dataset:\n",
        "  - High dimentionality brings problem in clustering phase: as K-Means is based on distance between objects, it becomes meaningless when we have a lot of features. Becouse a user profile is based on mean score on each Genre, i decided to remove those whit low presence in the dataset\n",
        "  - High numerosity, instead, brings problem in every process of the project: becouse of the size of the datasets, it was almost impossible to work while handling a lot of Giga of data as memory was not enough to perform certain operations\n",
        "\n",
        "After giving to every user a profile, there were another major problem: users do not all rate in the same way, someone is balanced giving votes in every range, someone else tends to rate only from 8 to 10, someone else never used those rates:\n",
        "  - Discern among the alternatives it's impossible, so i just assumed that scaling everyone with the same distribution of votes was the most reliable thing to do\n",
        "\n",
        "\n",
        "Then, i tested between K-Means, Agglomerative Clustering and DBSCAN supported by OPTICS:\n",
        "  - I tested all of them with the same data and picked up the one that gave me the best cluster quelity score, silhouette score for instance\n",
        "  - For every method i tested, quality score was not very good: in my opinion this is due to the fact that people's tastes are almost unique for each one: some may like various genres, while some may hate them but at the same time they may like any anime of the latter. Becouse of this, i think that some fuzzy clustering method may work better for this type of work\n",
        "\n",
        "In the end, after chosing K-Means (as it had better results), i tested all users using 10-fold cross-validation. The testing script works in this way:\n",
        "  - Split the dataset of users with generated profile in 10 folds\n",
        "  - For each iteration, use one fold as test Set and the others as training set\n",
        "  - After trained the model, remove some anime from the list of each user in the test set\n",
        "  - Use the remaining to profile them, and then put it into model to find the cluster where they fit the best\n",
        "  - Combining all the anime lists of training users in the same cluster and order them in count and mean score\n",
        "  - If the top ten anime in this list are present in the list of removed anime of the user under test, i measure how many of them are in this list from 0 to 10, recording a precision from 0% to 100%\n",
        "\n",
        "In this step there were some problems either: if a user under test, even after removing some of the anime, none of them is in the top ten list, the precision have a drop, becouse of the difference in anime number seen.\n",
        "Precision of each fold lay from 40 to 60%, with a mean score of 50%:\n",
        "this means that, in avarage, at least 5 of the anime in the recommanded list are really liked by the user\n",
        "\n",
        "I was pretty satisfied about that score, also becouse compared with other methods, it has a result better by 10 to 20 % points.\n",
        "\n",
        "Anyway, there is also something that may be improved:\n",
        "  - For instance, most famous anime are seen by everybody in all clustrers, so they tend to be recommended anyway no matter the cluster you are in\n",
        "  - Another problem is the quality of the clusters: the dataset may be explored better to find better solutions for clustering users: the dataset i used is 2 years old. As they updated the website some months ago, there is now a distinction between Genres and Themes: in my dataset they are all considered as Genres: this update can be exploited to cluster user first by genres and then by Themes, and mix up things somehow to see if there are major differences in clusters quality and final accuracy score\n",
        "\n",
        "I made a simple GUI program to use the model and API of anilist.co website:\n",
        "  - Putting your username, it retrieve your list, build your genre profile and recommend ten anime you haven't\n",
        "  - Anyway, in order for it to work, you have to rate some anime from the website\n",
        "  - Becouse the internal anime dataset it's 2 years old, newer anime don't count and are not recommended either"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e81e0442",
      "metadata": {},
      "source": [
        "------------------------\n",
        "The following is an implementation of the algorithm so that users can use it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "95c93440",
      "metadata": {},
      "source": [
        "In this part of code datasets for the recommended are created and saved in a dedicated folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jQGal0fuGRBS",
      "metadata": {
        "id": "jQGal0fuGRBS"
      },
      "outputs": [],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 50)\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import random as rm\n",
        "import math\n",
        "import gc\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['axes.grid'] = True\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "root = './Datasets'\n",
        "anime = pd.read_csv(root + '/animeCheckpoint2.csv', index_col='anime_id')\n",
        "ratings = pd.read_csv(root + '/ratingsCheckpoint2.csv').drop(columns = 'watching_status')\n",
        "ratingsReduced = pd.read_csv(root + '/ratingsReducedCheckpoint2.csv', index_col = 'user_id')\n",
        "\n",
        "ratings.drop(columns = ['Unnamed: 0'], inplace = True)\n",
        "# {1: 'Watching', 2: 'Completed', 3: 'On-Hold', 4: 'Dropped', 6: 'Planning'}\n",
        "genres = anime.drop(columns = ['episodes', 'type','name','score','popularity','members','favorites']).columns.to_numpy()\n",
        "animeReduced = anime.drop(columns = ['name','score','type','episodes','members','favorites','popularity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D1SJZcB6HXBP",
      "metadata": {
        "id": "D1SJZcB6HXBP"
      },
      "outputs": [],
      "source": [
        "def shuffleTrainTest(table = ratingsReduced, ttFactor = 0.6, verbose = True):\n",
        "\n",
        "  \"\"\"Shuffle unique users and split ratings in train and test set\n",
        "\n",
        "  Parameters:\n",
        "   > ttFactor (double) : default 0.6\n",
        "      How many users go in the training set (1 - ttFactor are in test set)\n",
        "\n",
        "  Returns:\n",
        "   > trainingSet, testSet\n",
        "  \n",
        "  \"\"\"\n",
        "  if((ttFactor > 0.8) or (ttFactor < 0)):\n",
        "    ttFactor = 0.6\n",
        "\n",
        "  uniqueUsers = table.index.unique().tolist()\n",
        "  rm.shuffle(uniqueUsers)\n",
        "\n",
        "  trainingUsers = []\n",
        "  testUsers = []\n",
        "\n",
        "  trainingSize = math.ceil(len(uniqueUsers) * ttFactor)\n",
        "  testSize = len(uniqueUsers) - trainingSize\n",
        "\n",
        "  for i in range(0,trainingSize):\n",
        "    trainingUsers.append(uniqueUsers[i])\n",
        "  for i in range(trainingSize, trainingSize + testSize):\n",
        "    testUsers.append(uniqueUsers[i])\n",
        "\n",
        "  trainingRatings = table[table.index.isin(trainingUsers)]\n",
        "  testRatings = table[table.index.isin(testUsers)]\n",
        "  if verbose:\n",
        "    print(f\"Training set: {len(trainingUsers)} users\\nTest set: {len(testUsers)} users\")\n",
        "    \n",
        "  return trainingRatings, testRatings\n",
        "\n",
        "def scale(table):\n",
        "  \"\"\"Scale the given table using StandardScaler\n",
        "    on the rows\n",
        "  \"\"\"\n",
        "  return pd.DataFrame(\n",
        "      StandardScaler().fit_transform(table.T),\n",
        "      index = table.columns, columns = table.index).T\n",
        "\n",
        "def mindiv(n, startn = 2):\n",
        "  for i in range(startn, n):\n",
        "    if n%i==0:\n",
        "      return i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XHQEcQTuHExB",
      "metadata": {
        "id": "XHQEcQTuHExB"
      },
      "outputs": [],
      "source": [
        "# Take the whole dataset\n",
        "trainingSet = ratingsReduced\n",
        "# Scale it\n",
        "scaledTrainingSet = scale(trainingSet)\n",
        "# Build clusters\n",
        "kmeans = KMeans(n_clusters = 13, random_state = 523, n_init = 10).fit(scaledTrainingSet)\n",
        "# Take the cluster for each training sample\n",
        "y_kmeans = kmeans.predict(scaledTrainingSet)\n",
        "# Create a table with user_id-cluster pairs\n",
        "clusters = pd.DataFrame(y_kmeans, columns = ['cluster'], index = scaledTrainingSet.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iEfkjH3VKg6S",
      "metadata": {
        "id": "iEfkjH3VKg6S"
      },
      "outputs": [],
      "source": [
        "dump(kmeans, root + '/recommenderFiles/model.joblib')\n",
        "anime.to_csv(root + '/recommenderFiles/anime.csv')\n",
        "clusters.to_csv(root + '/recommenderFiles/clusters.csv')\n",
        "ratings.to_csv(root + '/recommenderFiles/ratings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XbYnYjVwK1YI",
      "metadata": {
        "id": "XbYnYjVwK1YI"
      },
      "source": [
        "# RUN THE PROGRAM FROM HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "36baf5da",
      "metadata": {},
      "source": [
        "In this part of code the algorithm just take your list from `Anilist.co` and find suggestion for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f-7PTVLNzH",
      "metadata": {
        "id": "50f-7PTVLNzH"
      },
      "outputs": [],
      "source": [
        "from joblib import load\n",
        "import requests\n",
        "import threading\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', 50)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "root = './Datasets'\n",
        "def loadTask():\n",
        "  global ratings\n",
        "  ratings = pd.read_csv(root + '/recommenderFiles/ratings.csv').drop(columns = 'Unnamed: 0')\n",
        "  return\n",
        "\n",
        "t = threading.Thread(target = loadTask)\n",
        "t.start()\n",
        "\n",
        "ratings = None\n",
        "anime = pd.read_csv(root + '/recommenderFiles/anime.csv', index_col='anime_id')\n",
        "clusters = pd.read_csv(root + '/recommenderFiles/clusters.csv')\n",
        "genres = anime.drop(columns = ['episodes', 'type','name','score','popularity','members','favorites']).columns.to_numpy()\n",
        "animeReduced = anime.drop(columns = ['name','score','type','episodes','members','favorites','popularity'])\n",
        "\n",
        "kmeans = load(root + '/recommenderFiles/model.joblib')\n",
        "\n",
        "def scale(table):\n",
        "  \"\"\"Scale the given table using StandardScaler\n",
        "    on the rows\n",
        "  \"\"\"\n",
        "  return pd.DataFrame(\n",
        "      StandardScaler().fit_transform(table.T),\n",
        "      index = table.columns, columns = table.index).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oHjRMdfwo7bA",
      "metadata": {
        "id": "oHjRMdfwo7bA"
      },
      "outputs": [],
      "source": [
        "nick = 'PatataAliena' # my nickname on anilist.co site used to get the list\n",
        "\n",
        "query = '''\n",
        "query ($nickname: String) {\n",
        "    User (name: $nickname) {\n",
        "        id\n",
        "    }\n",
        "}\n",
        "'''\n",
        "variables = {\n",
        "    'nickname': nick\n",
        "}\n",
        "\n",
        "url = 'https://graphql.anilist.co'\n",
        "# API request\n",
        "response = requests.post(url, json={'query': query, 'variables': variables})\n",
        "\n",
        "if response.status_code == 404:\n",
        "  print(\"User not found\")\n",
        "elif response.status_code != 200:\n",
        "  print(f\"Error occurred: {response.status_code}\")\n",
        "else:\n",
        "  userID = response.json()['data']['User']['id']\n",
        "  print(f\"Found user: {userID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-NhgnVA0rwfV",
      "metadata": {
        "id": "-NhgnVA0rwfV"
      },
      "outputs": [],
      "source": [
        "query = '''\n",
        "query ($id: Int) {\n",
        "  MediaListCollection(userId: $id, type: ANIME) {\n",
        "    lists {\n",
        "      entries{\n",
        "        media {\n",
        "          idMal\n",
        "        }\n",
        "        score\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "'''\n",
        "variables = {\n",
        "    'id': userID\n",
        "}\n",
        "\n",
        "url = 'https://graphql.anilist.co'\n",
        "\n",
        "response = requests.post(url, json={'query': query, 'variables': variables})\n",
        "\n",
        "if response.status_code != 200:\n",
        "  print(f\"Error occurred: {response.json()}\")\n",
        "else:\n",
        "  edit = response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s4s68yy8yX7G",
      "metadata": {
        "id": "s4s68yy8yX7G"
      },
      "outputs": [],
      "source": [
        "myRatings = pd.DataFrame(columns = ['user_id', 'anime_id', 'rating'])\n",
        "for LIST in edit['data']['MediaListCollection']['lists']:\n",
        "  for ITEM in LIST['entries']:\n",
        "    idMal = ITEM['media']['idMal']\n",
        "    score = ITEM['score']\n",
        "    myRatings = myRatings.append({'user_id': 0, 'anime_id': idMal,'rating': score}, ignore_index = True)\n",
        "\n",
        "myRatings = myRatings[myRatings.anime_id.isin(anime.index)]\n",
        "myRatings['user_id'] = myRatings['user_id'].astype(int)\n",
        "myRatings['anime_id'] = myRatings['anime_id'].astype(int)\n",
        "myList = myRatings.anime_id.tolist()\n",
        "myRatings = myRatings[myRatings['rating'] > 0]\n",
        "\n",
        "t.join()\n",
        "ratingsSupp = ratings.append(myRatings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fs4XIA42hFLe",
      "metadata": {
        "id": "Fs4XIA42hFLe"
      },
      "outputs": [],
      "source": [
        "# UserID\n",
        "x = 0\n",
        "\n",
        "# Build the genre profile for the user\n",
        "temp = myRatings.join(animeReduced, on = 'anime_id').drop(columns = ['anime_id'])\n",
        "temp[genres] = temp[genres].T.multiply(temp['rating']).T\n",
        "temp[genres] = temp[genres].replace(0, np.nan)\n",
        "temp = temp.drop(columns = 'rating').groupby(by = 'user_id').mean().replace(np.nan,0)\n",
        "\n",
        "# Also scale the user rating list\n",
        "scaledTest = scale(temp)\n",
        "# Cluster label for tested testUser\n",
        "label = kmeans.predict(scaledTest)\n",
        "\n",
        "# USers in same cluster\n",
        "nearestUsers = clusters[clusters.cluster == label[0]].index\n",
        "temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NRXsOdANL10c",
      "metadata": {
        "id": "NRXsOdANL10c"
      },
      "outputs": [],
      "source": [
        "hiddenList = [121,5081,2167,4181]\n",
        "# Hide things i am not really interested\n",
        "\n",
        "nearestUsers = clusters[clusters['cluster'] == label[0]].user_id\n",
        "usersList = ratings.loc[ratings.user_id.isin(nearestUsers)].drop(columns = 'user_id')\n",
        "usersList['count'] = 1\n",
        "meanList = usersList.groupby(by = 'anime_id').sum()\n",
        "meanList['rating'] = meanList['rating'] / meanList['count']\n",
        "finalList = meanList.join(anime[['name', 'members']])\n",
        "topTen = finalList[['name', 'count', 'rating', 'members']].sort_values(by = ['count', 'rating', 'members'], ascending = False)\n",
        "mask = (~(ratingsSupp.anime_id.isin(myList)) & ~(ratingsSupp.anime_id.isin(hiddenList)) & (ratingsSupp.user_id.isin(nearestUsers)))\n",
        "result = topTen[topTen.index.isin(ratingsSupp.loc[mask].anime_id)].head(10)\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "70c6b47bbdbb53ba4d6f078e17c80d4dae8e1351f1f474fa5ad05b0251799bf5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
